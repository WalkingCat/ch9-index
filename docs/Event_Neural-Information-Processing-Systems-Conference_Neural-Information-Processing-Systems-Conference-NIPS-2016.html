<head><link rel='stylesheet' href='styles.css'></head><body class='content'>
<nobr class='title-container'><h2><a href='https://web.archive.org/web/2020/https://channel9.msdn.com/Events/Neural-Information-Processing-Systems-Conference/Neural-Information-Processing-Systems-Conference-NIPS-2016' target='_blank'><img src='logo_archive-sm.png' width=24 height=24></a> <span class='title'>Event - Neural-Information-Processing-Systems-Conference_Neural-Information-Processing-Systems-Conference-NIPS-2016</span></h2></nobr><br/>
<nobr class='vtitle-container'><a href='https://web.archive.org/web/2020/https://channel9.msdn.com/Events/Neural-Information-Processing-Systems-Conference/Neural-Information-Processing-Systems-Conference-NIPS-2016/Large-Scale-Optimization-Beyond-Stochastic-Gradient-Descent-and-Convexity' target='_blank'><img src='logo_archive-sm.png' width=24 height=24></a> <span class='vtitle'><a href='https://sec.ch9.ms/ch9/0e6d/c86246c0-3d71-42cd-a146-469a546b0e6d/BachSra_mid.mp4' target='_blank'>Large-Scale Optimization: Beyond Stochastic Gradient Descent and Convexity</a> [1:55:55] [2017/06/08]</span></nobr><img class='vthumb' src='https://sec.ch9.ms/ch9/0e6d/c86246c0-3d71-42cd-a146-469a546b0e6d/BachSra_220.jpg'/><div class='vdesc'>Stochastic optimization lies at the heart of machine learning, and its cornerstone is stochastic gradient descent (SGD), a staple introduced over 60 years ago! Recent years have, however, brought an&#8230;</div><br clear='left'/><br/>
<nobr class='vtitle-container'><a href='https://web.archive.org/web/2020/https://channel9.msdn.com/Events/Neural-Information-Processing-Systems-Conference/Neural-Information-Processing-Systems-Conference-NIPS-2016/Machine-Learning-and-the-Law-Symposium-Session-1' target='_blank'><img src='logo_archive-sm.png' width=24 height=24></a> <span class='vtitle'><a href='https://sec.ch9.ms/ch9/cb39/60ad7aa7-eb61-4f39-bc07-9a41d24dcb39/symposium2Law1_mid.mp4' target='_blank'>Machine Learning and the Law Symposium Session 1</a> [1:53:30] [2017/04/25]</span></nobr><img class='vthumb' src='https://sec.ch9.ms/ch9/cb39/60ad7aa7-eb61-4f39-bc07-9a41d24dcb39/symposium2Law1_220.jpg'/><div class='vdesc'>Advances in machine learning and artificial intelligence mean that predictions and decisions of algorithms are already in use in many important situations under legal or regulatory control, and this&#8230;</div><br clear='left'/><br/>
<nobr class='vtitle-container'><a href='https://web.archive.org/web/2020/https://channel9.msdn.com/Events/Neural-Information-Processing-Systems-Conference/Neural-Information-Processing-Systems-Conference-NIPS-2016/Nuts-and-Bolts-of-Building-Applications-using-Deep-Learning' target='_blank'><img src='logo_archive-sm.png' width=24 height=24></a> <span class='vtitle'><a href='https://sec.ch9.ms/ch9/443b/a2a1c2f7-9740-4af8-a3e2-9faa63ff443b/Ng_mid.mp4' target='_blank'>Nuts and Bolts of Building Applications using Deep Learning</a> [2:06:54] [2017/04/25]</span></nobr><img class='vthumb' src='https://sec.ch9.ms/ch9/443b/a2a1c2f7-9740-4af8-a3e2-9faa63ff443b/Ng_220.jpg'/><div class='vdesc'>How do you get deep learning to work in your business, product, or scientific study? The rise of highly scalable deep learning techniques is changing how you can best approach AI problems. This&#8230;</div><br clear='left'/><br/>
<nobr class='vtitle-container'><a href='https://web.archive.org/web/2020/https://channel9.msdn.com/Events/Neural-Information-Processing-Systems-Conference/Neural-Information-Processing-Systems-Conference-NIPS-2016/Machine-Learning-and-the-Law-Symposium-Session-3' target='_blank'><img src='logo_archive-sm.png' width=24 height=24></a> <span class='vtitle'><a href='https://sec.ch9.ms/ch9/29e4/7bdee2ed-c49c-4fc2-88ae-4d332e7729e4/SymposiumMachineLawPart3_mid.mp4' target='_blank'>Machine Learning and the Law Symposium Session 3</a> [1:31:28] [2017/04/25]</span></nobr><img class='vthumb' src='https://sec.ch9.ms/ch9/29e4/7bdee2ed-c49c-4fc2-88ae-4d332e7729e4/SymposiumMachineLawPart3_220.jpg'/><div class='vdesc'>Advances in machine learning and artificial intelligence mean that predictions and decisions of algorithms are already in use in many important situations under legal or regulatory control, and this&#8230;</div><br clear='left'/><br/>
<nobr class='vtitle-container'><a href='https://web.archive.org/web/2020/https://channel9.msdn.com/Events/Neural-Information-Processing-Systems-Conference/Neural-Information-Processing-Systems-Conference-NIPS-2016/Machine-Learning-and-the-Law-Symposium-Session-2' target='_blank'><img src='logo_archive-sm.png' width=24 height=24></a> <span class='vtitle'><a href='https://sec.ch9.ms/ch9/327f/2f568dd7-55ee-4082-b6b7-9601a3a0327f/MLLaw2_mid.mp4' target='_blank'>Machine Learning and the Law Symposium Session 2</a> [1:47:33] [2017/04/24]</span></nobr><img class='vthumb' src='https://sec.ch9.ms/ch9/327f/2f568dd7-55ee-4082-b6b7-9601a3a0327f/MLLaw2_220.jpg'/><div class='vdesc'>Advances in machine learning and artificial intelligence mean that predictions and decisions of algorithms are already in use in many important situations under legal or regulatory control, and this&#8230;</div><br clear='left'/><br/>
<nobr class='vtitle-container'><a href='https://web.archive.org/web/2020/https://channel9.msdn.com/Events/Neural-Information-Processing-Systems-Conference/Neural-Information-Processing-Systems-Conference-NIPS-2016/Recurrent-Neural-Networks-and-Other-Machines-that-Learn-Algorithms-Symposium-Session-3' target='_blank'><img src='logo_archive-sm.png' width=24 height=24></a> <span class='vtitle'><a href='https://sec.ch9.ms/ch9/da8b/e68e7a19-38dd-4b50-8bc2-2f733223da8b/RecurrentNeuralNetworksandOtherMachinesthatLearnA_mid.mp4' target='_blank'>Recurrent Neural Networks and Other Machines that Learn Algorithms Symposium Session 3</a> [1:28:15] [2017/03/17]</span></nobr><img class='vthumb' src='https://sec.ch9.ms/ch9/da8b/e68e7a19-38dd-4b50-8bc2-2f733223da8b/RecurrentNeuralNetworksandOtherMachinesthatLearnA_220.jpg'/><div class='vdesc'>Soon after the birth of modern computer science in the 1930s, two fundamental questions arose: 1. How can computers learn useful programs from experience, as opposed to being programmed by human&#8230;</div><br clear='left'/><br/>
<nobr class='vtitle-container'><a href='https://web.archive.org/web/2020/https://channel9.msdn.com/Events/Neural-Information-Processing-Systems-Conference/Neural-Information-Processing-Systems-Conference-NIPS-2016/Recurrent-Neural-Networks-and-Other-Machines-that-Learn-Algorithms-Symposium-Session-1' target='_blank'><img src='logo_archive-sm.png' width=24 height=24></a> <span class='vtitle'><a href='https://sec.ch9.ms/ch9/51c7/4e5e36ba-af76-4f32-a342-c736211f51c7/RecurrentNeuralNetworksandOtherMachinesthatLearnA_mid.mp4' target='_blank'>Recurrent Neural Networks and Other Machines that Learn Algorithms Symposium Session 1</a> [1:53:55] [2017/03/02]</span></nobr><img class='vthumb' src='https://sec.ch9.ms/ch9/51c7/4e5e36ba-af76-4f32-a342-c736211f51c7/RecurrentNeuralNetworksandOtherMachinesthatLearnA_220.jpg'/><div class='vdesc'>Soon after the birth of modern computer science in the 1930s, two fundamental questions arose: 1. How can computers learn useful programs from experience, as opposed to being programmed by human&#8230;</div><br clear='left'/><br/>
<nobr class='vtitle-container'><a href='https://web.archive.org/web/2020/https://channel9.msdn.com/Events/Neural-Information-Processing-Systems-Conference/Neural-Information-Processing-Systems-Conference-NIPS-2016/Recurrent-Neural-Networks-and-Other-Machines-that-Learn-Algorithms-Symposium-Session-2' target='_blank'><img src='logo_archive-sm.png' width=24 height=24></a> <span class='vtitle'><a href='https://sec.ch9.ms/ch9/efab/9720aa69-e4b1-4593-95a7-baba0595efab/Recurrent2_mid.mp4' target='_blank'>Recurrent Neural Networks and Other Machines that Learn Algorithms Symposium Session 2</a> [1:46:35] [2017/02/28]</span></nobr><img class='vthumb' src='https://sec.ch9.ms/ch9/efab/9720aa69-e4b1-4593-95a7-baba0595efab/Recurrent2_220.jpg'/><div class='vdesc'>Soon after the birth of modern computer science in the 1930s, two fundamental questions arose: 1. How can computers learn useful programs from experience, as opposed to being programmed by human&#8230;</div><br clear='left'/><br/>
<nobr class='vtitle-container'><a href='https://web.archive.org/web/2020/https://channel9.msdn.com/Events/Neural-Information-Processing-Systems-Conference/Neural-Information-Processing-Systems-Conference-NIPS-2016/Deep-Reinforcement-Learning-Through-Policy-Optimization' target='_blank'><img src='logo_archive-sm.png' width=24 height=24></a> <span class='vtitle'><a href='https://sec.ch9.ms/ch9/045d/94e12d70-3c8f-467f-a5f6-d5b1f865045d/DeepReinforcementLearningThroughPolicyOptimizatio_mid.mp4' target='_blank'>Deep Reinforcement Learning Through Policy Optimization</a> [1:59:06] [2017/01/24]</span></nobr><img class='vthumb' src='https://sec.ch9.ms/ch9/045d/94e12d70-3c8f-467f-a5f6-d5b1f865045d/DeepReinforcementLearningThroughPolicyOptimizatio_220.jpg'/><div class='vdesc'>Reinforcement Learning (Deep RL) has seen several breakthroughs in recent years. In this tutorial we will focus on recent advances in Deep RL through policy gradient methods and actor critic methods.&#8230;</div><br clear='left'/><br/>
<nobr class='vtitle-container'><a href='https://web.archive.org/web/2020/https://channel9.msdn.com/Events/Neural-Information-Processing-Systems-Conference/Neural-Information-Processing-Systems-Conference-NIPS-2016/Theory-and-Algorithms-for-Forecasting-Non-Stationary-Time-Series' target='_blank'><img src='logo_archive-sm.png' width=24 height=24></a> <span class='vtitle'><a href='https://sec.ch9.ms/ch9/388d/feeee505-9d2a-4b8e-9df5-133dfa3f388d/VitalyMehryar_mid.mp4' target='_blank'>Theory and Algorithms for Forecasting Non-Stationary Time Series</a> [1:45:04] [2017/01/24]</span></nobr><img class='vthumb' src='https://sec.ch9.ms/ch9/388d/feeee505-9d2a-4b8e-9df5-133dfa3f388d/VitalyMehryar_220.jpg'/><div class='vdesc'>Time series appear in a variety of key real-world applications such as signal processing, including audio and video processing; the analysis of natural phenomena such as local weather, global&#8230;</div><br clear='left'/><br/>
<nobr class='vtitle-container'><a href='https://web.archive.org/web/2020/https://channel9.msdn.com/Events/Neural-Information-Processing-Systems-Conference/Neural-Information-Processing-Systems-Conference-NIPS-2016/Crowdsourcing-Beyond-Label-Generation' target='_blank'><img src='logo_archive-sm.png' width=24 height=24></a> <span class='vtitle'><a href='https://sec.ch9.ms/ch9/efe7/1dfa805f-fd73-47a5-91fe-2fbf49c7efe7/Vaughan_mid.mp4' target='_blank'>Crowdsourcing: Beyond Label Generation</a> [1:50:18] [2017/01/24]</span></nobr><img class='vthumb' src='https://sec.ch9.ms/ch9/efe7/1dfa805f-fd73-47a5-91fe-2fbf49c7efe7/Vaughan_220.jpg'/><div class='vdesc'>This tutorial will showcase some of the most innovative uses of crowdsourcing that have emerged in the past few years. While some have clear and immediate benefits to machine learning, we will also&#8230;</div><br clear='left'/><br/>
<nobr class='vtitle-container'><a href='https://web.archive.org/web/2020/https://channel9.msdn.com/Events/Neural-Information-Processing-Systems-Conference/Neural-Information-Processing-Systems-Conference-NIPS-2016/ML-Foundations-and-Methods-for-Precision-Medicine-and-Healthcare' target='_blank'><img src='logo_archive-sm.png' width=24 height=24></a> <span class='vtitle'><a href='https://sec.ch9.ms/ch9/48f0/5bbb949b-dec2-4daf-b0e6-6ac7cc4848f0/SariaSchulam_mid.mp4' target='_blank'>ML Foundations and Methods for Precision Medicine and Healthcare</a> [2:08:17] [2017/01/24]</span></nobr><img class='vthumb' src='https://sec.ch9.ms/ch9/48f0/5bbb949b-dec2-4daf-b0e6-6ac7cc4848f0/SariaSchulam_220.jpg'/><div class='vdesc'>Electronic health records and high throughput measurement technologies are changing the practice of healthcare to become more algorithmic and data-driven. This offers an exciting opportunity for&#8230;</div><br clear='left'/><br/>
<nobr class='vtitle-container'><a href='https://web.archive.org/web/2020/https://channel9.msdn.com/Events/Neural-Information-Processing-Systems-Conference/Neural-Information-Processing-Systems-Conference-NIPS-2016/Variational-Inference-Foundations-and-Modern-Methods' target='_blank'><img src='logo_archive-sm.png' width=24 height=24></a> <span class='vtitle'><a href='https://sec.ch9.ms/ch9/ce50/89dc37af-7514-4e40-9c19-affec1b6ce50/BleiRanganathMohamed_mid.mp4' target='_blank'>Variational Inference: Foundations and Modern Methods</a> [1:53:04] [2017/01/24]</span></nobr><img class='vthumb' src='https://sec.ch9.ms/ch9/ce50/89dc37af-7514-4e40-9c19-affec1b6ce50/BleiRanganathMohamed_220.jpg'/><div class='vdesc'>One of the core problems of modern statistics and machine learning is to approximate difficult-to-compute probability distributions. This problem is especially important in probabilistic modeling,&#8230;</div><br clear='left'/><br/>
<nobr class='vtitle-container'><a href='https://web.archive.org/web/2020/https://channel9.msdn.com/Events/Neural-Information-Processing-Systems-Conference/Neural-Information-Processing-Systems-Conference-NIPS-2016/Generative-Adversarial-Networks' target='_blank'><img src='logo_archive-sm.png' width=24 height=24></a> <span class='vtitle'><a href='https://sec.ch9.ms/ch9/6b3d/57930795-7f62-4218-8e18-888623426b3d/Goodfellow_mid.mp4' target='_blank'>Generative Adversarial Networks</a> [1:55:53] [2017/01/24]</span></nobr><img class='vthumb' src='https://sec.ch9.ms/ch9/6b3d/57930795-7f62-4218-8e18-888623426b3d/Goodfellow_220.jpg'/><div class='vdesc'>Generative adversarial networks (GANs) are a recently introduced class of generative models, designed to produce realistic samples. This tutorial is intended to be accessible to an audience who has no&#8230;</div><br clear='left'/><br/>
<nobr class='vtitle-container'><a href='https://web.archive.org/web/2020/https://channel9.msdn.com/Events/Neural-Information-Processing-Systems-Conference/Neural-Information-Processing-Systems-Conference-NIPS-2016/Predictive-Learning' target='_blank'><img src='logo_archive-sm.png' width=24 height=24></a> <span class='vtitle'><a href='https://sec.ch9.ms/ch9/5c00/860be7da-9672-4813-9530-9e9369085c00/Yann_mid.mp4' target='_blank'>Predictive Learning</a> [0:56:52] [2017/01/24]</span></nobr><img class='vthumb' src='https://sec.ch9.ms/ch9/5c00/860be7da-9672-4813-9530-9e9369085c00/Yann_220.jpg'/><div class='vdesc'>Deep learning has been at the root of significant progress in many application areas, such as computer perception and natural language processing. But almost all of these systems currently use&#8230;</div><br clear='left'/><br/>
<nobr class='vtitle-container'><a href='https://web.archive.org/web/2020/https://channel9.msdn.com/Events/Neural-Information-Processing-Systems-Conference/Neural-Information-Processing-Systems-Conference-NIPS-2016/Intelligent-Biosphere' target='_blank'><img src='logo_archive-sm.png' width=24 height=24></a> <span class='vtitle'><a href='https://sec.ch9.ms/ch9/39dd/797ba269-15aa-47b5-b830-92b93da739dd/Purves_mid.mp4' target='_blank'>Intelligent Biosphere</a> [0:49:42] [2017/01/24]</span></nobr><img class='vthumb' src='https://sec.ch9.ms/ch9/39dd/797ba269-15aa-47b5-b830-92b93da739dd/Purves_220.jpg'/><div class='vdesc'>The biosphere is a stupendously complex and poorly understood system, which we depend on for our survival, and which we are attacking on every front. Worrying. But what has that got to do with machine&#8230;</div><br clear='left'/><br/>
<nobr class='vtitle-container'><a href='https://web.archive.org/web/2020/https://channel9.msdn.com/Events/Neural-Information-Processing-Systems-Conference/Neural-Information-Processing-Systems-Conference-NIPS-2016/Value-Iteration-Networks' target='_blank'><img src='logo_archive-sm.png' width=24 height=24></a> <span class='vtitle'><a href='https://sec.ch9.ms/ch9/653e/6d6df5bc-bdea-4564-8119-459fdb1b653e/VIN_mid.mp4' target='_blank'>Value Iteration Networks</a> [0:19:40] [2017/01/24]</span></nobr><img class='vthumb' src='https://sec.ch9.ms/ch9/653e/6d6df5bc-bdea-4564-8119-459fdb1b653e/VIN_220.jpg'/><div class='vdesc'>We introduce the value iteration network (VIN): a fully differentiable neural network with a `planning module' embedded within. VINs can learn to plan, and are suitable for predicting outcomes that&#8230;</div><br clear='left'/><br/>
<nobr class='vtitle-container'><a href='https://web.archive.org/web/2020/https://channel9.msdn.com/Events/Neural-Information-Processing-Systems-Conference/Neural-Information-Processing-Systems-Conference-NIPS-2016/Tractable-Operations-for-Arithmetic-Circuits-of-Probabilistic-Models' target='_blank'><img src='logo_archive-sm.png' width=24 height=24></a> <span class='vtitle'><a href='https://sec.ch9.ms/ch9/06c6/cd30c70c-8cb0-4711-996c-9512d1fa06c6/ShenChoiDarwiche_mid.mp4' target='_blank'>Tractable Operations for Arithmetic Circuits of Probabilistic Models</a> [0:19:15] [2017/01/24]</span></nobr><img class='vthumb' src='https://sec.ch9.ms/ch9/06c6/cd30c70c-8cb0-4711-996c-9512d1fa06c6/ShenChoiDarwiche_220.jpg'/><div class='vdesc'>We consider tractable representations of probability distributions and the polytime operations they support. In particular, we consider a recently proposed arithmetic circuit representation, the&#8230;</div><br clear='left'/><br/>
<nobr class='vtitle-container'><a href='https://web.archive.org/web/2020/https://channel9.msdn.com/Events/Neural-Information-Processing-Systems-Conference/Neural-Information-Processing-Systems-Conference-NIPS-2016/Testing-for-Differences-in-Gaussian-Graphical-Models-Applications-to-Brain-Connectivity' target='_blank'><img src='logo_archive-sm.png' width=24 height=24></a> <span class='vtitle'><a href='https://sec.ch9.ms/ch9/030a/096e21a0-1f87-44d8-8ee2-623aa731030a/Belilovsky_mid.mp4' target='_blank'>Testing for Differences in Gaussian Graphical Models: Applications to Brain Connectivity</a> [0:16:51] [2017/01/24]</span></nobr><img class='vthumb' src='https://sec.ch9.ms/ch9/030a/096e21a0-1f87-44d8-8ee2-623aa731030a/Belilovsky_220.jpg'/><div class='vdesc'>Functional brain networks are well described and estimated from data with Gaussian Graphical Models (GGMs), e.g.\ using sparse inverse covariance estimators. Comparing functional connectivity of&#8230;</div><br clear='left'/><br/>
<nobr class='vtitle-container'><a href='https://web.archive.org/web/2020/https://channel9.msdn.com/Events/Neural-Information-Processing-Systems-Conference/Neural-Information-Processing-Systems-Conference-NIPS-2016/SDP-Relaxation-with-Randomized-Rounding-for-Energy-Disaggregation' target='_blank'><img src='logo_archive-sm.png' width=24 height=24></a> <span class='vtitle'><a href='https://sec.ch9.ms/ch9/ce7a/a2aff902-6a98-4e57-90f9-cc526aaece7a/Shaloudegi_mid.mp4' target='_blank'>SDP Relaxation with Randomized Rounding for Energy Disaggregation</a> [0:21:36] [2017/01/24]</span></nobr><img class='vthumb' src='https://sec.ch9.ms/ch9/ce7a/a2aff902-6a98-4e57-90f9-cc526aaece7a/Shaloudegi_220.jpg'/><div class='vdesc'>We develop a scalable, computationally efficient method for the task of energy disaggregation for home appliance monitoring. In this problem the goal is to estimate the energy consumption of each&#8230;</div><br clear='left'/><br/>
<nobr class='vtitle-container'><a href='https://web.archive.org/web/2020/https://channel9.msdn.com/Events/Neural-Information-Processing-Systems-Conference/Neural-Information-Processing-Systems-Conference-NIPS-2016/Bayesian-Intermittent-Demand-Forecasting-for-Large-Inventories' target='_blank'><img src='logo_archive-sm.png' width=24 height=24></a> <span class='vtitle'><a href='https://sec.ch9.ms/ch9/83be/6a8f841a-0831-497e-bb38-eb3d1cf583be/Seeger_mid.mp4' target='_blank'>Bayesian Intermittent Demand Forecasting for Large Inventories</a> [0:18:01] [2017/01/24]</span></nobr><img class='vthumb' src='https://sec.ch9.ms/ch9/83be/6a8f841a-0831-497e-bb38-eb3d1cf583be/Seeger_220.jpg'/><div class='vdesc'>We present a scalable and robust Bayesian method for demand forecasting in the context of a large e-commerce platform, paying special attention to intermittent and bursty target statistics. Inference&#8230;</div><br clear='left'/><br/>
<nobr class='vtitle-container'><a href='https://web.archive.org/web/2020/https://channel9.msdn.com/Events/Neural-Information-Processing-Systems-Conference/Neural-Information-Processing-Systems-Conference-NIPS-2016/Synthesis-of-MCMC-and-Belief-Propagation' target='_blank'><img src='logo_archive-sm.png' width=24 height=24></a> <span class='vtitle'><a href='https://sec.ch9.ms/ch9/4f64/8e298fe2-589e-4f16-961c-3a37d7e14f64/Ahn_mid.mp4' target='_blank'>Synthesis of MCMC and Belief Propagation</a> [0:17:26] [2017/01/24]</span></nobr><img class='vthumb' src='https://sec.ch9.ms/ch9/4f64/8e298fe2-589e-4f16-961c-3a37d7e14f64/Ahn_220.jpg'/><div class='vdesc'>Markov Chain Monte Carlo (MCMC) and Belief Propagation (BP) are the most popular algorithms for computational inference in Graphical Models (GM). In principle, MCMC is an exact probabilistic method&#8230;</div><br clear='left'/><br/>
<nobr class='vtitle-container'><a href='https://web.archive.org/web/2020/https://channel9.msdn.com/Events/Neural-Information-Processing-Systems-Conference/Neural-Information-Processing-Systems-Conference-NIPS-2016/Deep-Learning-for-Predicting-Human-Strategic-Behavior' target='_blank'><img src='logo_archive-sm.png' width=24 height=24></a> <span class='vtitle'><a href='https://sec.ch9.ms/ch9/e713/7446746e-1923-4e18-9a73-1ff4ed32e713/DeepLearningforPredictingHumanStrategicBehavior_mid.mp4' target='_blank'>Deep Learning for Predicting Human Strategic Behavior</a> [0:19:19] [2017/01/24]</span></nobr><img class='vthumb' src='https://sec.ch9.ms/ch9/e713/7446746e-1923-4e18-9a73-1ff4ed32e713/DeepLearningforPredictingHumanStrategicBehavior_220.jpg'/><div class='vdesc'>Predicting the behavior of human participants in strategic settings is an important problem in many domains. Most existing work either assumes that participants are perfectly rational, or attempts to&#8230;</div><br clear='left'/><br/>
<nobr class='vtitle-container'><a href='https://web.archive.org/web/2020/https://channel9.msdn.com/Events/Neural-Information-Processing-Systems-Conference/Neural-Information-Processing-Systems-Conference-NIPS-2016/Using-Fast-Weights-to-Attend-to-the-Recent-Past' target='_blank'><img src='logo_archive-sm.png' width=24 height=24></a> <span class='vtitle'><a href='https://sec.ch9.ms/ch9/4faf/8c363686-df2d-4d30-bf50-8269acf34faf/Bu_mid.mp4' target='_blank'>Using Fast Weights to Attend to the Recent Past</a> [0:21:02] [2017/01/24]</span></nobr><img class='vthumb' src='https://sec.ch9.ms/ch9/4faf/8c363686-df2d-4d30-bf50-8269acf34faf/Bu_220.jpg'/><div class='vdesc'>Until recently, research on artificial neural networks was largely restricted to systems with only two types of variable: Neural activities that represent the current or recent input and weights that&#8230;</div><br clear='left'/><br/>
<nobr class='vtitle-container'><a href='https://web.archive.org/web/2020/https://channel9.msdn.com/Events/Neural-Information-Processing-Systems-Conference/Neural-Information-Processing-Systems-Conference-NIPS-2016/Sequential-Neural-Models-with-Stochastic-Layers' target='_blank'><img src='logo_archive-sm.png' width=24 height=24></a> <span class='vtitle'><a href='https://sec.ch9.ms/ch9/e7e0/b24ba1d4-0307-41dc-a2c1-8b75f9b5e7e0/Fraccaro_mid.mp4' target='_blank'>Sequential Neural Models with Stochastic Layers</a> [0:20:18] [2017/01/24]</span></nobr><img class='vthumb' src='https://sec.ch9.ms/ch9/e7e0/b24ba1d4-0307-41dc-a2c1-8b75f9b5e7e0/Fraccaro_220.jpg'/><div class='vdesc'>How can we efficiently propagate uncertainty in a latent state representation with recurrent neural networks? This paper introduces stochastic recurrent neural networks which glue a deterministic&#8230;</div><br clear='left'/><br/>
<nobr class='vtitle-container'><a href='https://web.archive.org/web/2020/https://channel9.msdn.com/Events/Neural-Information-Processing-Systems-Conference/Neural-Information-Processing-Systems-Conference-NIPS-2016/Phased-LSTM-Accelerating-Recurrent-Network-Training-for-Long-or-Event-based-Sequences' target='_blank'><img src='logo_archive-sm.png' width=24 height=24></a> <span class='vtitle'><a href='https://sec.ch9.ms/ch9/6bf5/880282e4-7e13-4046-81a5-6ca89ab96bf5/Niel_mid.mp4' target='_blank'>Phased LSTM: Accelerating Recurrent Network Training for Long or Event-based Sequences</a> [0:20:55] [2017/01/24]</span></nobr><img class='vthumb' src='https://sec.ch9.ms/ch9/6bf5/880282e4-7e13-4046-81a5-6ca89ab96bf5/Niel_220.jpg'/><div class='vdesc'>Recurrent Neural Networks (RNNs) have become the state-of-the-art choice for extracting patterns from temporal sequences. Current RNN models are ill suited to process irregularly sampled data&#8230;</div><br clear='left'/><br/>
<nobr class='vtitle-container'><a href='https://web.archive.org/web/2020/https://channel9.msdn.com/Events/Neural-Information-Processing-Systems-Conference/Neural-Information-Processing-Systems-Conference-NIPS-2016/Graphons-mergeons-and-so-on' target='_blank'><img src='logo_archive-sm.png' width=24 height=24></a> <span class='vtitle'><a href='https://sec.ch9.ms/ch9/bde8/f3c2f6ed-09f4-4ad0-ba2f-e4bd0293bde8/Eldridge_mid.mp4' target='_blank'>Graphons, mergeons, and so on!</a> [0:17:10] [2017/01/24]</span></nobr><img class='vthumb' src='https://sec.ch9.ms/ch9/bde8/f3c2f6ed-09f4-4ad0-ba2f-e4bd0293bde8/Eldridge_220.jpg'/><div class='vdesc'>In this work we develop a theory of hierarchical clustering for graphs. Our modelling assumption is that graphs are sampled from a graphon, which is a powerful and general model for generating graphs&#8230;</div><br clear='left'/><br/>
<nobr class='vtitle-container'><a href='https://web.archive.org/web/2020/https://channel9.msdn.com/Events/Neural-Information-Processing-Systems-Conference/Neural-Information-Processing-Systems-Conference-NIPS-2016/Hierarchical-Clustering-via-Spreading-Metrics' target='_blank'><img src='logo_archive-sm.png' width=24 height=24></a> <span class='vtitle'><a href='https://sec.ch9.ms/ch9/7cd5/c95e4d32-3701-4f01-aa44-f9f47e0d7cd5/Roy_mid.mp4' target='_blank'>Hierarchical Clustering via Spreading Metrics</a> [0:17:40] [2017/01/24]</span></nobr><img class='vthumb' src='https://sec.ch9.ms/ch9/7cd5/c95e4d32-3701-4f01-aa44-f9f47e0d7cd5/Roy_220.jpg'/><div class='vdesc'>We study the cost function for hierarchical clusterings introduced by [Dasgupta, 2015] where hierarchies are treated as first-class objects rather than deriving their cost from projections into flat&#8230;</div><br clear='left'/><br/>
<nobr class='vtitle-container'><a href='https://web.archive.org/web/2020/https://channel9.msdn.com/Events/Neural-Information-Processing-Systems-Conference/Neural-Information-Processing-Systems-Conference-NIPS-2016/Clustering-with-Same-Cluster-Queries' target='_blank'><img src='logo_archive-sm.png' width=24 height=24></a> <span class='vtitle'><a href='https://sec.ch9.ms/ch9/2a94/75494bf6-d567-4e85-a6c2-ef017adf2a94/Ashtiani_mid.mp4' target='_blank'>Clustering with Same-Cluster Queries</a> [0:18:05] [2017/01/24]</span></nobr><img class='vthumb' src='https://sec.ch9.ms/ch9/2a94/75494bf6-d567-4e85-a6c2-ef017adf2a94/Ashtiani_220.jpg'/><div class='vdesc'>We propose a framework for Semi-Supervised Active Clustering framework (SSAC), where the learner is allowed to interact with a domain expert, asking whether two given instances belong to the same&#8230;</div><br clear='left'/><br/>
<nobr class='vtitle-container'><a href='https://web.archive.org/web/2020/https://channel9.msdn.com/Events/Neural-Information-Processing-Systems-Conference/Neural-Information-Processing-Systems-Conference-NIPS-2016/Unsupervised-Feature-Extraction-by-Time-Contrastive-Learning-and-Nonlinear-ICA' target='_blank'><img src='logo_archive-sm.png' width=24 height=24></a> <span class='vtitle'><a href='https://sec.ch9.ms/ch9/4095/a0070345-b608-4990-a10b-9cf7e4404095/Hyvarinen_mid.mp4' target='_blank'>Unsupervised Feature Extraction by Time-Contrastive Learning and Nonlinear ICA</a> [0:22:50] [2017/01/24]</span></nobr><img class='vthumb' src='https://sec.ch9.ms/ch9/4095/a0070345-b608-4990-a10b-9cf7e4404095/Hyvarinen_220.jpg'/><div class='vdesc'>Nonlinear independent component analysis (ICA) provides an appealing framework for unsupervised feature learning, but the models proposed so far are not identifiable. Here, we first propose a new&#8230;</div><br clear='left'/><br/>
<nobr class='vtitle-container'><a href='https://web.archive.org/web/2020/https://channel9.msdn.com/Events/Neural-Information-Processing-Systems-Conference/Neural-Information-Processing-Systems-Conference-NIPS-2016/Fast-and-Provably-Good-Seedings-for-k-Means' target='_blank'><img src='logo_archive-sm.png' width=24 height=24></a> <span class='vtitle'><a href='https://sec.ch9.ms/ch9/1f8a/3c5492ca-180a-41f4-aa60-09755a0c1f8a/Bachem_mid.mp4' target='_blank'>Fast and Provably Good Seedings for k-Means</a> [0:19:44] [2017/01/24]</span></nobr><img class='vthumb' src='https://sec.ch9.ms/ch9/1f8a/3c5492ca-180a-41f4-aa60-09755a0c1f8a/Bachem_220.jpg'/><div class='vdesc'>Seeding - the task of finding initial cluster centers - is critical in obtaining high-quality clusterings for k-Means. However, k-means&#43;&#43; seeding, the state of the art algorithm, does not&#8230;</div><br clear='left'/><br/>
<nobr class='vtitle-container'><a href='https://web.archive.org/web/2020/https://channel9.msdn.com/Events/Neural-Information-Processing-Systems-Conference/Neural-Information-Processing-Systems-Conference-NIPS-2016/Supervised-learning-through-the-lens-of-compression' target='_blank'><img src='logo_archive-sm.png' width=24 height=24></a> <span class='vtitle'><a href='https://sec.ch9.ms/ch9/d13b/50cca4f8-bfe9-47f9-8a41-ca5e3699d13b/Moran_mid.mp4' target='_blank'>Supervised learning through the lens of compression</a> [0:16:52] [2017/01/24]</span></nobr><img class='vthumb' src='https://sec.ch9.ms/ch9/d13b/50cca4f8-bfe9-47f9-8a41-ca5e3699d13b/Moran_220.jpg'/><div class='vdesc'>This work continues the study of the relationship between sample compression schemes and statistical learning, which has been mostly investigated within the framework of binary classification. We&#8230;</div><br clear='left'/><br/>
<nobr class='vtitle-container'><a href='https://web.archive.org/web/2020/https://channel9.msdn.com/Events/Neural-Information-Processing-Systems-Conference/Neural-Information-Processing-Systems-Conference-NIPS-2016/MetaGrad-Multiple-Learning-Rates-in-Online-Learning' target='_blank'><img src='logo_archive-sm.png' width=24 height=24></a> <span class='vtitle'><a href='https://sec.ch9.ms/ch9/1435/da1cb27e-22b3-4f65-bb63-0cc697441435/Koolen_mid.mp4' target='_blank'>MetaGrad: Multiple Learning Rates in Online Learning</a> [0:16:36] [2017/01/24]</span></nobr><img class='vthumb' src='https://sec.ch9.ms/ch9/1435/da1cb27e-22b3-4f65-bb63-0cc697441435/Koolen_220.jpg'/><div class='vdesc'>In online convex optimization it is well known that certain subclasses of objective functions are much easier than arbitrary convex functions. We are interested in designing adaptive methods that can&#8230;</div><br clear='left'/><br/>
<nobr class='vtitle-container'><a href='https://web.archive.org/web/2020/https://channel9.msdn.com/Events/Neural-Information-Processing-Systems-Conference/Neural-Information-Processing-Systems-Conference-NIPS-2016/Blazing-the-trails-before-beating-the-path-Sample-efficient-Monte-Carlo-planning' target='_blank'><img src='logo_archive-sm.png' width=24 height=24></a> <span class='vtitle'><a href='https://sec.ch9.ms/ch9/5bc8/c5253f5e-8d0b-443b-8b4e-8c621cce5bc8/Grill_mid.mp4' target='_blank'>Blazing the trails before beating the path: Sample-efficient Monte-Carlo planning</a> [0:15:52] [2017/01/24]</span></nobr><img class='vthumb' src='https://sec.ch9.ms/ch9/5bc8/c5253f5e-8d0b-443b-8b4e-8c621cce5bc8/Grill_220.jpg'/><div class='vdesc'>We study the sampling-based planning problem in Markov decision processes (MDPs) that we can access only through a generative model, usually referred to as Monte-Carlo planning. Our objective is to&#8230;</div><br clear='left'/><br/>
<nobr class='vtitle-container'><a href='https://web.archive.org/web/2020/https://channel9.msdn.com/Events/Neural-Information-Processing-Systems-Conference/Neural-Information-Processing-Systems-Conference-NIPS-2016/Global-Analysis-of-Expectation-Maximization-for-Mixtures-of-Two-Gaussians' target='_blank'><img src='logo_archive-sm.png' width=24 height=24></a> <span class='vtitle'><a href='https://sec.ch9.ms/ch9/5fa6/ac070b92-dbb5-4b3f-ad0f-6a5d6af95fa6/Xu_mid.mp4' target='_blank'>Global Analysis of Expectation Maximization for Mixtures of Two Gaussians</a> [0:18:58] [2017/01/24]</span></nobr><img class='vthumb' src='https://sec.ch9.ms/ch9/5fa6/ac070b92-dbb5-4b3f-ad0f-6a5d6af95fa6/Xu_220.jpg'/><div class='vdesc'>Expectation Maximization (EM) is among the most popular algorithms for estimating parameters of statistical models. However, EM, which is an iterative algorithm based on the maximum likelihood&#8230;</div><br clear='left'/><br/>
<nobr class='vtitle-container'><a href='https://web.archive.org/web/2020/https://channel9.msdn.com/Events/Neural-Information-Processing-Systems-Conference/Neural-Information-Processing-Systems-Conference-NIPS-2016/Machine-Learning-and-Likelihood-Free-Inference-in-Particle-Physics' target='_blank'><img src='logo_archive-sm.png' width=24 height=24></a> <span class='vtitle'><a href='https://sec.ch9.ms/ch9/ce38/540802a3-6367-47f2-aa26-950fc840ce38/Cranmer_mid.mp4' target='_blank'>Machine Learning and Likelihood-Free Inference in Particle Physics</a> [0:50:37] [2017/01/24]</span></nobr><img class='vthumb' src='https://sec.ch9.ms/ch9/ce38/540802a3-6367-47f2-aa26-950fc840ce38/Cranmer_220.jpg'/><div class='vdesc'>Particle physics aims to answer profound questions about the fundamental building blocks of the Universe through enormous data sets collected at experiments like the Large Hadron Collider at CERN.&#8230;</div><br clear='left'/><br/>
<nobr class='vtitle-container'><a href='https://web.archive.org/web/2020/https://channel9.msdn.com/Events/Neural-Information-Processing-Systems-Conference/Neural-Information-Processing-Systems-Conference-NIPS-2016/Matrix-Completion-has-No-Spurious-Local-Minimum' target='_blank'><img src='logo_archive-sm.png' width=24 height=24></a> <span class='vtitle'><a href='https://sec.ch9.ms/ch9/d101/0399980d-f6c3-4f04-ac17-58dfae41d101/Ma_mid.mp4' target='_blank'>Matrix Completion has No Spurious Local Minimum</a> [0:18:26] [2017/01/24]</span></nobr><img class='vthumb' src='https://sec.ch9.ms/ch9/d101/0399980d-f6c3-4f04-ac17-58dfae41d101/Ma_220.jpg'/><div class='vdesc'>Matrix completion is a basic machine learning problem that has wide applications, especially in collaborative filtering and recommender systems. Simple non-convex optimization algorithms are popular&#8230;</div><br clear='left'/><br/>
<nobr class='vtitle-container'><a href='https://web.archive.org/web/2020/https://channel9.msdn.com/Events/Neural-Information-Processing-Systems-Conference/Neural-Information-Processing-Systems-Conference-NIPS-2016/Large-Scale-Price-Optimization-via-Network-Flow' target='_blank'><img src='logo_archive-sm.png' width=24 height=24></a> <span class='vtitle'><a href='https://sec.ch9.ms/ch9/83f5/7e060b2e-bf42-436c-8335-28b476e783f5/Ito_mid.mp4' target='_blank'>Large-Scale Price Optimization via Network Flow</a> [0:18:59] [2017/01/24]</span></nobr><img class='vthumb' src='https://sec.ch9.ms/ch9/83f5/7e060b2e-bf42-436c-8335-28b476e783f5/Ito_220.jpg'/><div class='vdesc'>This paper deals with price optimization, which is to find the best pricing strategy that maximizes revenue or profit, on the basis of demand forecasting models. Though recent advances in regression&#8230;</div><br clear='left'/><br/>
<nobr class='vtitle-container'><a href='https://web.archive.org/web/2020/https://channel9.msdn.com/Events/Neural-Information-Processing-Systems-Conference/Neural-Information-Processing-Systems-Conference-NIPS-2016/Visual-Dynamics-Probabilistic-Future-Frame-Synthesis-via-Cross-Convolutional-Networks' target='_blank'><img src='logo_archive-sm.png' width=24 height=24></a> <span class='vtitle'><a href='https://sec.ch9.ms/ch9/3def/61f3afb4-7a8c-4148-a11c-1439145a3def/Xue_mid.mp4' target='_blank'>Visual Dynamics: Probabilistic Future Frame Synthesis via Cross Convolutional Networks</a> [0:18:17] [2017/01/24]</span></nobr><img class='vthumb' src='https://sec.ch9.ms/ch9/3def/61f3afb4-7a8c-4148-a11c-1439145a3def/Xue_220.jpg'/><div class='vdesc'>We study the problem of synthesizing a number of likely future frames from a single input image. In contrast to traditional methods, which have tackled this problem in a deterministic or&#8230;</div><br clear='left'/><br/>
<nobr class='vtitle-container'><a href='https://web.archive.org/web/2020/https://channel9.msdn.com/Events/Neural-Information-Processing-Systems-Conference/Neural-Information-Processing-Systems-Conference-NIPS-2016/Supervised-Word-Movers-Distance' target='_blank'><img src='logo_archive-sm.png' width=24 height=24></a> <span class='vtitle'><a href='https://sec.ch9.ms/ch9/5488/23ac60e6-bd89-4a33-ab77-821abf8d5488/Kusner_mid.mp4' target='_blank'>Supervised Word Mover's Distance</a> [0:21:30] [2017/01/24]</span></nobr><img class='vthumb' src='https://sec.ch9.ms/ch9/5488/23ac60e6-bd89-4a33-ab77-821abf8d5488/Kusner_220.jpg'/><div class='vdesc'>Accurately measuring the similarity between text documents lies at the core of many real world applications of machine learning. These include web-search ranking, document recommendation,&#8230;</div><br clear='left'/><br/>
<nobr class='vtitle-container'><a href='https://web.archive.org/web/2020/https://channel9.msdn.com/Events/Neural-Information-Processing-Systems-Conference/Neural-Information-Processing-Systems-Conference-NIPS-2016/Beyond-Exchangeability-The-Chinese-Voting-Process' target='_blank'><img src='logo_archive-sm.png' width=24 height=24></a> <span class='vtitle'><a href='https://sec.ch9.ms/ch9/ad1c/3a6cf931-61bc-455c-98c6-7209b002ad1c/Lee_mid.mp4' target='_blank'>Beyond Exchangeability: The Chinese Voting Process</a> [0:19:34] [2017/01/24]</span></nobr><img class='vthumb' src='https://sec.ch9.ms/ch9/ad1c/3a6cf931-61bc-455c-98c6-7209b002ad1c/Lee_220.jpg'/><div class='vdesc'>Many online communities present user-contributed responses, such as reviews of products and answers to questions. User-provided helpfulness votes can highlight the most useful responses, but voting is&#8230;</div><br clear='left'/><br/>
<nobr class='vtitle-container'><a href='https://web.archive.org/web/2020/https://channel9.msdn.com/Events/Neural-Information-Processing-Systems-Conference/Neural-Information-Processing-Systems-Conference-NIPS-2016/Protein-contact-prediction-from-amino-acid-co-evolution-using-convolutional-networks-for-graph-value' target='_blank'><img src='logo_archive-sm.png' width=24 height=24></a> <span class='vtitle'><a href='https://sec.ch9.ms/ch9/6fc3/94ceae0a-ff4c-45a6-b8d7-e2476bc56fc3/Golkov_mid.mp4' target='_blank'>Protein contact prediction from amino acid co-evolution using convolutional networks for&#8230;</a> [0:20:12] [2017/01/24]</span></nobr><img class='vthumb' src='https://sec.ch9.ms/ch9/6fc3/94ceae0a-ff4c-45a6-b8d7-e2476bc56fc3/Golkov_220.jpg'/><div class='vdesc'>Proteins are the &quot;building blocks of life&quot;, the most abundant organic molecules, and the central focus of most areas of biomedicine. Protein structure is strongly related to protein&#8230;</div><br clear='left'/><br/>
<nobr class='vtitle-container'><a href='https://web.archive.org/web/2020/https://channel9.msdn.com/Events/Neural-Information-Processing-Systems-Conference/Neural-Information-Processing-Systems-Conference-NIPS-2016/Deep-Learning-without-Poor-Local-Minima' target='_blank'><img src='logo_archive-sm.png' width=24 height=24></a> <span class='vtitle'><a href='https://sec.ch9.ms/ch9/dc83/25529631-0674-4590-a0c6-4d0b5e3ddc83/Kawaguchi_mid.mp4' target='_blank'>Deep Learning without Poor Local Minima</a> [0:19:19] [2017/01/24]</span></nobr><img class='vthumb' src='https://sec.ch9.ms/ch9/dc83/25529631-0674-4590-a0c6-4d0b5e3ddc83/Kawaguchi_220.jpg'/><div class='vdesc'>In this paper, we prove a conjecture published in 1989 and also partially address an open problem announced at the Conference on Learning Theory (COLT) 2015. For an expected loss function of a deep&#8230;</div><br clear='left'/><br/>
<nobr class='vtitle-container'><a href='https://web.archive.org/web/2020/https://channel9.msdn.com/Events/Neural-Information-Processing-Systems-Conference/Neural-Information-Processing-Systems-Conference-NIPS-2016/Learning-to-Poke-by-Poking-Experiential-Learning-of-Intuitive-Physics' target='_blank'><img src='logo_archive-sm.png' width=24 height=24></a> <span class='vtitle'><a href='https://sec.ch9.ms/ch9/2df4/801b969c-dc0b-4b5f-b033-7beca2bb2df4/Agrawal_mid.mp4' target='_blank'>Learning to Poke by Poking: Experiential Learning of Intuitive Physics</a> [0:21:00] [2017/01/24]</span></nobr><img class='vthumb' src='https://sec.ch9.ms/ch9/2df4/801b969c-dc0b-4b5f-b033-7beca2bb2df4/Agrawal_220.jpg'/><div class='vdesc'>We investigate an experiential learning paradigm for acquiring an internal model of intuitive physics. Our model is evaluated on a real-world robotic manipulation task that requires displacing objects&#8230;</div><br clear='left'/><br/>
<nobr class='vtitle-container'><a href='https://web.archive.org/web/2020/https://channel9.msdn.com/Events/Neural-Information-Processing-Systems-Conference/Neural-Information-Processing-Systems-Conference-NIPS-2016/Learning-What-and-Where-to-Draw' target='_blank'><img src='logo_archive-sm.png' width=24 height=24></a> <span class='vtitle'><a href='https://sec.ch9.ms/ch9/3e9a/087db78b-767a-49b6-b7db-38099bc43e9a/Reed_mid.mp4' target='_blank'>Learning What and Where to Draw</a> [0:21:37] [2017/01/24]</span></nobr><img class='vthumb' src='https://sec.ch9.ms/ch9/3e9a/087db78b-767a-49b6-b7db-38099bc43e9a/Reed_220.jpg'/><div class='vdesc'>Generative Adversarial Networks (GANs) have recently demonstrated the capability to synthesize compelling real-world images, such as room interiors, album covers, manga, faces, birds, and flowers.&#8230;</div><br clear='left'/><br/>
<nobr class='vtitle-container'><a href='https://web.archive.org/web/2020/https://channel9.msdn.com/Events/Neural-Information-Processing-Systems-Conference/Neural-Information-Processing-Systems-Conference-NIPS-2016/Weight-Normalization-A-Simple-Reparameterization-to-Accelerate-Training-of-Deep-Neural-Networks' target='_blank'><img src='logo_archive-sm.png' width=24 height=24></a> <span class='vtitle'><a href='https://sec.ch9.ms/ch9/5473/29f6b73d-d9ff-4a02-838b-9db4bcf45473/Salimans_mid.mp4' target='_blank'>Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks</a> [0:22:55] [2017/01/24]</span></nobr><img class='vthumb' src='https://sec.ch9.ms/ch9/5473/29f6b73d-d9ff-4a02-838b-9db4bcf45473/Salimans_220.jpg'/><div class='vdesc'>We present weight normalization: a reparameterization of the weight vectors in a neural network that decouples the length of those weight vectors from their direction. By reparameterizing the weights&#8230;</div><br clear='left'/><br/>
<nobr class='vtitle-container'><a href='https://web.archive.org/web/2020/https://channel9.msdn.com/Events/Neural-Information-Processing-Systems-Conference/Neural-Information-Processing-Systems-Conference-NIPS-2016/Achieving-the-KS-threshold-in-the-general-stochastic-block-model-with-linearized-acyclic-belief-prop' target='_blank'><img src='logo_archive-sm.png' width=24 height=24></a> <span class='vtitle'><a href='https://sec.ch9.ms/ch9/7940/39bcddee-1773-4224-b38a-ab8714067940/Abbe_mid.mp4' target='_blank'>Achieving the KS threshold in the general stochastic block model with linearized acyclic belief&#8230;</a> [0:15:04] [2017/01/24]</span></nobr><img class='vthumb' src='https://sec.ch9.ms/ch9/7940/39bcddee-1773-4224-b38a-ab8714067940/Abbe_220.jpg'/><div class='vdesc'>The stochastic block model (SBM) has long been studied in machine learning and network science as a canonical model for clustering and community detection. In the recent years, new developments have&#8230;</div><br clear='left'/><br/>
<nobr class='vtitle-container'><a href='https://web.archive.org/web/2020/https://channel9.msdn.com/Events/Neural-Information-Processing-Systems-Conference/Neural-Information-Processing-Systems-Conference-NIPS-2016/Orthogonal-Random-Features' target='_blank'><img src='logo_archive-sm.png' width=24 height=24></a> <span class='vtitle'><a href='https://sec.ch9.ms/ch9/7dc0/e4738613-73a0-4acc-91e6-82117b207dc0/Suresh_mid.mp4' target='_blank'>Orthogonal Random Features</a> [0:18:42] [2017/01/24]</span></nobr><img class='vthumb' src='https://sec.ch9.ms/ch9/7dc0/e4738613-73a0-4acc-91e6-82117b207dc0/Suresh_220.jpg'/><div class='vdesc'>We present an intriguing discovery related to Random Fourier Features: replacing multiplication by a random Gaussian matrix with multiplication by a properly scaled random orthogonal matrix&#8230;</div><br clear='left'/><br/>
<nobr class='vtitle-container'><a href='https://web.archive.org/web/2020/https://channel9.msdn.com/Events/Neural-Information-Processing-Systems-Conference/Neural-Information-Processing-Systems-Conference-NIPS-2016/Poisson-Gamma-dynamical-systems' target='_blank'><img src='logo_archive-sm.png' width=24 height=24></a> <span class='vtitle'><a href='https://sec.ch9.ms/ch9/25eb/3338a523-df71-4c9a-943f-2ba297ee25eb/Schein_mid.mp4' target='_blank'>Poisson-Gamma dynamical systems</a> [0:16:40] [2017/01/24]</span></nobr><img class='vthumb' src='https://sec.ch9.ms/ch9/25eb/3338a523-df71-4c9a-943f-2ba297ee25eb/Schein_220.jpg'/><div class='vdesc'>This paper presents a dynamical system based on the Poisson-Gamma construction for sequentially observed multivariate count data. Inherent to the model is a novel Bayesian nonparametric prior that&#8230;</div><br clear='left'/><br/>
<nobr class='vtitle-container'><a href='https://web.archive.org/web/2020/https://channel9.msdn.com/Events/Neural-Information-Processing-Systems-Conference/Neural-Information-Processing-Systems-Conference-NIPS-2016/The-Multiscale-Laplacian-Graph-Kernel' target='_blank'><img src='logo_archive-sm.png' width=24 height=24></a> <span class='vtitle'><a href='https://sec.ch9.ms/ch9/0e46/e3cc11ec-2a13-4ec5-a9b2-2455c51d0e46/Kondor_mid.mp4' target='_blank'>The Multiscale Laplacian Graph Kernel</a> [0:20:34] [2017/01/24]</span></nobr><img class='vthumb' src='https://sec.ch9.ms/ch9/0e46/e3cc11ec-2a13-4ec5-a9b2-2455c51d0e46/Kondor_220.jpg'/><div class='vdesc'>Many real world graphs, such as the graphs of molecules, exhibit structure at multiple different scales, but most existing kernels between graphs are either purely local or purely global in character.&#8230;</div><br clear='left'/><br/>
<nobr class='vtitle-container'><a href='https://web.archive.org/web/2020/https://channel9.msdn.com/Events/Neural-Information-Processing-Systems-Conference/Neural-Information-Processing-Systems-Conference-NIPS-2016/Stochastic-Online-AUC-Maximization' target='_blank'><img src='logo_archive-sm.png' width=24 height=24></a> <span class='vtitle'><a href='https://sec.ch9.ms/ch9/4fe6/39255c10-d7fd-4f88-8c98-5da3913f4fe6/Ying_mid.mp4' target='_blank'>Stochastic Online AUC Maximization</a> [0:14:40] [2017/01/24]</span></nobr><img class='vthumb' src='https://sec.ch9.ms/ch9/4fe6/39255c10-d7fd-4f88-8c98-5da3913f4fe6/Ying_220.jpg'/><div class='vdesc'>Area under ROC (AUC) is a metric which is widely used for measuring the classification performance for imbalanced data. It is of theoretical and practical interest to develop online learning&#8230;</div><br clear='left'/><br/>
<nobr class='vtitle-container'><a href='https://web.archive.org/web/2020/https://channel9.msdn.com/Events/Neural-Information-Processing-Systems-Conference/Neural-Information-Processing-Systems-Conference-NIPS-2016/Without-Replacement-Sampling-for-Stochastic-Gradient-Methods' target='_blank'><img src='logo_archive-sm.png' width=24 height=24></a> <span class='vtitle'><a href='https://sec.ch9.ms/ch9/1d29/da5fed90-9fb1-4505-8fa0-918cad131d29/Shamir_mid.mp4' target='_blank'>Without-Replacement Sampling for Stochastic Gradient Methods</a> [0:19:55] [2017/01/24]</span></nobr><img class='vthumb' src='https://sec.ch9.ms/ch9/1d29/da5fed90-9fb1-4505-8fa0-918cad131d29/Shamir_220.jpg'/><div class='vdesc'>Stochastic gradient methods for machine learning and optimization problems are usually analyzed assuming data points are sampled with replacement. In contrast, sampling without replacement is far less&#8230;</div><br clear='left'/><br/>
<nobr class='vtitle-container'><a href='https://web.archive.org/web/2020/https://channel9.msdn.com/Events/Neural-Information-Processing-Systems-Conference/Neural-Information-Processing-Systems-Conference-NIPS-2016/Regularized-Nonlinear-Acceleration' target='_blank'><img src='logo_archive-sm.png' width=24 height=24></a> <span class='vtitle'><a href='https://sec.ch9.ms/ch9/c4f9/3f3c40de-0be8-44a6-9f8a-965fcfbfc4f9/Scieur_mid.mp4' target='_blank'>Regularized Nonlinear Acceleration</a> [0:18:52] [2017/01/24]</span></nobr><img class='vthumb' src='https://sec.ch9.ms/ch9/c4f9/3f3c40de-0be8-44a6-9f8a-965fcfbfc4f9/Scieur_220.jpg'/><div class='vdesc'>We describe a convergence acceleration technique for generic optimization problems. Our scheme computes estimates of the optimum from a nonlinear average of the iterates produced by any optimization&#8230;</div><br clear='left'/><br/>
<nobr class='vtitle-container'><a href='https://web.archive.org/web/2020/https://channel9.msdn.com/Events/Neural-Information-Processing-Systems-Conference/Neural-Information-Processing-Systems-Conference-NIPS-2016/Generalization-of-ERM-in-Stochastic-Convex-Optimization-The-Dimension-Strikes-Back' target='_blank'><img src='logo_archive-sm.png' width=24 height=24></a> <span class='vtitle'><a href='https://sec.ch9.ms/ch9/e500/aa07311c-1c41-4365-bf8b-618877ede500/Feldman_mid.mp4' target='_blank'>Generalization of ERM in Stochastic Convex Optimization: The Dimension Strikes Back</a> [0:16:21] [2017/01/24]</span></nobr><img class='vthumb' src='https://sec.ch9.ms/ch9/e500/aa07311c-1c41-4365-bf8b-618877ede500/Feldman_220.jpg'/><div class='vdesc'>In stochastic convex optimization the goal is to minimize a convex function $F(x) \doteq \E{f\sim D}[f(x)]overaconvexset  \K \subset \R^dwhere  Dissomeunknowndistributionandeach &#8230;</div><br clear='left'/><br/>
<nobr class='vtitle-container'><a href='https://web.archive.org/web/2020/https://channel9.msdn.com/Events/Neural-Information-Processing-Systems-Conference/Neural-Information-Processing-Systems-Conference-NIPS-2016/Bayesian-Optimization-with-Robust-Bayesian-Neural-Networks' target='_blank'><img src='logo_archive-sm.png' width=24 height=24></a> <span class='vtitle'><a href='https://sec.ch9.ms/ch9/6fc0/569f3d1d-1982-4b88-90ef-76f694b76fc0/Springenberg_mid.mp4' target='_blank'>Bayesian Optimization with Robust Bayesian Neural Networks</a> [0:14:50] [2017/01/24]</span></nobr><img class='vthumb' src='https://sec.ch9.ms/ch9/6fc0/569f3d1d-1982-4b88-90ef-76f694b76fc0/Springenberg_220.jpg'/><div class='vdesc'>Bayesian optimization is a prominent method for optimizing expensive to evaluate black-box functions that is prominently applied to tuning the hyperparameters of machine learning algorithms. Despite&#8230;</div><br clear='left'/><br/>
<nobr class='vtitle-container'><a href='https://web.archive.org/web/2020/https://channel9.msdn.com/Events/Neural-Information-Processing-Systems-Conference/Neural-Information-Processing-Systems-Conference-NIPS-2016/Learning-About-the-Brain-Neuroimaging-and-Beyond' target='_blank'><img src='logo_archive-sm.png' width=24 height=24></a> <span class='vtitle'><a href='https://sec.ch9.ms/ch9/f85f/0ea9e0b3-8e8c-4832-8c34-d53ad4e1f85f/Rish_mid.mp4' target='_blank'>Learning About the Brain: Neuroimaging and Beyond</a> [0:51:37] [2017/01/24]</span></nobr><img class='vthumb' src='https://sec.ch9.ms/ch9/f85f/0ea9e0b3-8e8c-4832-8c34-d53ad4e1f85f/Rish_220.jpg'/><div class='vdesc'>Quantifying mental states and identifying &quot;statistical biomarkers&quot; of mental disorders from neuroimaging data is an exciting and rapidly growing research area at the intersection of&#8230;</div><br clear='left'/><br/>
<nobr class='vtitle-container'><a href='https://web.archive.org/web/2020/https://channel9.msdn.com/Events/Neural-Information-Processing-Systems-Conference/Neural-Information-Processing-Systems-Conference-NIPS-2016/Reproducible-Research-the-Case-of-the-Human-Microbiome' target='_blank'><img src='logo_archive-sm.png' width=24 height=24></a> <span class='vtitle'><a href='https://sec.ch9.ms/ch9/04ea/275bc129-e0f0-4beb-8324-88b8d40b04ea/Holmes_mid.mp4' target='_blank'>Reproducible Research: the Case of the Human Microbiome</a> [0:55:53] [2017/01/24]</span></nobr><img class='vthumb' src='https://sec.ch9.ms/ch9/04ea/275bc129-e0f0-4beb-8324-88b8d40b04ea/Holmes_220.jpg'/><div class='vdesc'>Modern data sets usually present multiple levels of heterogeneity, some apparent such as the necessity of combining trees, graphs, contingency tables and continuous covariates, others concern latent&#8230;</div><br clear='left'/><br/>
<nobr class='vtitle-container'><a href='https://web.archive.org/web/2020/https://channel9.msdn.com/Events/Neural-Information-Processing-Systems-Conference/Neural-Information-Processing-Systems-Conference-NIPS-2016/Interpretable-Distribution-Features-with-Maximum-Testing-Power' target='_blank'><img src='logo_archive-sm.png' width=24 height=24></a> <span class='vtitle'><a href='https://sec.ch9.ms/ch9/7dc2/9219e4d4-5495-4217-bc14-6f71e4fb7dc2/Jitkrittum_mid.mp4' target='_blank'>Interpretable Distribution Features with Maximum Testing Power</a> [0:22:45] [2017/01/24]</span></nobr><img class='vthumb' src='https://sec.ch9.ms/ch9/7dc2/9219e4d4-5495-4217-bc14-6f71e4fb7dc2/Jitkrittum_220.jpg'/><div class='vdesc'>Two semimetrics on probability distributions are proposed, given as the sum of differences of expectations of analytic functions evaluated at spatial or frequency locations (i.e, features). The&#8230;</div><br clear='left'/><br/>
<nobr class='vtitle-container'><a href='https://web.archive.org/web/2020/https://channel9.msdn.com/Events/Neural-Information-Processing-Systems-Conference/Neural-Information-Processing-Systems-Conference-NIPS-2016/Examples-are-not-enough-learn-to-criticize-Criticism-for-Interpretability' target='_blank'><img src='logo_archive-sm.png' width=24 height=24></a> <span class='vtitle'><a href='https://sec.ch9.ms/ch9/a683/259e30d7-4ba9-4728-9c79-7c5ca982a683/Kim_mid.mp4' target='_blank'>Examples are not enough, learn to criticize! Criticism for Interpretability </a> [0:20:39] [2017/01/24]</span></nobr><img class='vthumb' src='https://sec.ch9.ms/ch9/a683/259e30d7-4ba9-4728-9c79-7c5ca982a683/Kim_220.jpg'/><div class='vdesc'>Example-based explanations are widely used in the effort to improve the interpretability of highly complex distributions. However, prototypes alone are rarely sufficient to represent the gist of the&#8230;</div><br clear='left'/><br/>
<nobr class='vtitle-container'><a href='https://web.archive.org/web/2020/https://channel9.msdn.com/Events/Neural-Information-Processing-Systems-Conference/Neural-Information-Processing-Systems-Conference-NIPS-2016/Showing-versus-doing-Teaching-by-demonstration' target='_blank'><img src='logo_archive-sm.png' width=24 height=24></a> <span class='vtitle'><a href='https://sec.ch9.ms/ch9/9bb7/0cd5e272-42b6-4b11-9d25-6f4943699bb7/Ho_mid.mp4' target='_blank'>Showing versus doing: Teaching by demonstration</a> [0:19:03] [2017/01/24]</span></nobr><img class='vthumb' src='https://sec.ch9.ms/ch9/9bb7/0cd5e272-42b6-4b11-9d25-6f4943699bb7/Ho_220.jpg'/><div class='vdesc'>People often learn from others' demonstrations, and classic inverse reinforcement learning (IRL) algorithms have brought us closer to realizing this capacity in machines. In contrast, teaching by&#8230;</div><br clear='left'/><br/>
<nobr class='vtitle-container'><a href='https://web.archive.org/web/2020/https://channel9.msdn.com/Events/Neural-Information-Processing-Systems-Conference/Neural-Information-Processing-Systems-Conference-NIPS-2016/Relevant-sparse-codes-with-variational-information-bottleneck' target='_blank'><img src='logo_archive-sm.png' width=24 height=24></a> <span class='vtitle'><a href='https://sec.ch9.ms/ch9/884e/fedded64-92f6-4dee-b6b1-95a0484a884e/Chalk_mid.mp4' target='_blank'>Relevant sparse codes with variational information bottleneck</a> [0:17:53] [2017/01/24]</span></nobr><img class='vthumb' src='https://sec.ch9.ms/ch9/884e/fedded64-92f6-4dee-b6b1-95a0484a884e/Chalk_220.jpg'/><div class='vdesc'>In many applications, it is desirable to extract only the relevant aspects of data. A principled way to do this is the information bottleneck (IB) method, where one seeks a code that maximises&#8230;</div><br clear='left'/><br/>
<nobr class='vtitle-container'><a href='https://web.archive.org/web/2020/https://channel9.msdn.com/Events/Neural-Information-Processing-Systems-Conference/Neural-Information-Processing-Systems-Conference-NIPS-2016/Dense-Associative-Memory-for-Pattern-Recognition' target='_blank'><img src='logo_archive-sm.png' width=24 height=24></a> <span class='vtitle'><a href='https://sec.ch9.ms/ch9/1467/b2fe2859-127f-486e-b26e-71072aed1467/Krotov_mid.mp4' target='_blank'>Dense Associative Memory for Pattern Recognition</a> [0:24:11] [2017/01/24]</span></nobr><img class='vthumb' src='https://sec.ch9.ms/ch9/1467/b2fe2859-127f-486e-b26e-71072aed1467/Krotov_220.jpg'/><div class='vdesc'>A model of associative memory is studied, which stores and reliably retrieves many more patterns than the number of neurons in the network. We propose a simple duality between this dense associative&#8230;</div><br clear='left'/><br/>
<nobr class='vtitle-container'><a href='https://web.archive.org/web/2020/https://channel9.msdn.com/Events/Neural-Information-Processing-Systems-Conference/Neural-Information-Processing-Systems-Conference-NIPS-2016/Deep-Learning-Symposium-Session-3' target='_blank'><img src='logo_archive-sm.png' width=24 height=24></a> <span class='vtitle'><a href='https://sec.ch9.ms/ch9/44e7/5d9ee1be-6b8e-4fac-bb74-05c4540044e7/DeepLearning3_mid.mp4' target='_blank'>Deep Learning Symposium Session 3</a> [1:21:49] [2017/01/24]</span></nobr><img class='vthumb' src='https://sec.ch9.ms/ch9/44e7/5d9ee1be-6b8e-4fac-bb74-05c4540044e7/DeepLearning3_220.jpg'/><div class='vdesc'>Deep Learning algorithms attempt to discover good representations, at multiple levels of abstraction. Deep Learning is a topic of broad interest, both to researchers who develop new algorithms and&#8230;</div><br clear='left'/><br/>
<nobr class='vtitle-container'><a href='https://web.archive.org/web/2020/https://channel9.msdn.com/Events/Neural-Information-Processing-Systems-Conference/Neural-Information-Processing-Systems-Conference-NIPS-2016/Deep-Learning-Symposium-Session-2' target='_blank'><img src='logo_archive-sm.png' width=24 height=24></a> <span class='vtitle'><a href='https://sec.ch9.ms/ch9/5624/6a31a3d0-daf4-4869-ad32-c3e04c625624/DeepLearning2_mid.mp4' target='_blank'>Deep Learning Symposium Session 2</a> [1:05:19] [2017/01/24]</span></nobr><img class='vthumb' src='https://sec.ch9.ms/ch9/5624/6a31a3d0-daf4-4869-ad32-c3e04c625624/DeepLearning2_220.jpg'/><div class='vdesc'>Deep Learning algorithms attempt to discover good representations, at multiple levels of abstraction. Deep Learning is a topic of broad interest, both to researchers who develop new algorithms and&#8230;</div><br clear='left'/><br/>
<nobr class='vtitle-container'><a href='https://web.archive.org/web/2020/https://channel9.msdn.com/Events/Neural-Information-Processing-Systems-Conference/Neural-Information-Processing-Systems-Conference-NIPS-2016/Deep-Learning-Symposium-Session-1' target='_blank'><img src='logo_archive-sm.png' width=24 height=24></a> <span class='vtitle'><a href='https://sec.ch9.ms/ch9/9306/0e3de764-7e49-454c-be8c-96e8897b9306/DeepLearning1_mid.mp4' target='_blank'>Deep Learning Symposium Session 1</a> [1:54:39] [2017/01/24]</span></nobr><img class='vthumb' src='https://sec.ch9.ms/ch9/9306/0e3de764-7e49-454c-be8c-96e8897b9306/DeepLearning1_220.jpg'/><div class='vdesc'>Deep Learning algorithms attempt to discover good representations, at multiple levels of abstraction. Deep Learning is a topic of broad interest, both to researchers who develop new algorithms and&#8230;</div><br clear='left'/><br/>
